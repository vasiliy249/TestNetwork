Аннотация

В данной статье рассказывается о децентрализованной системе из класса распределённых сетей,
работающих подобно хэш-таблице (DHT) с формально доказанной согласованностью (?) и
производительностью в среде, склонной к сбоям. Описанная система направляет запросы и
размещает узлы с использованием топологического пространства, метрика которого основана на
операции XOR, что упрощает алгоритм и его формальное доказательство. Топология данного
пространства обладает тем свойством, что каждый обмен сообщениями не только передаёт, но и
усиливает полезную информацию. Kademlia использует эту информацию в системе отправки
параллельных асинхронных сообщений-запросов, которая допускает сбои узлов и, тем самым, не
подвержена увеличению временной задержки запроса.

Введение
Эта работа рассказывает о Kademlia – одноранговой распределённой хэш-таблице (DHT). Kademlia
имеет ряд достоинств, одновременное наличие которых нет ни у одной прежней реализации DHT.
Одного из них - минимизация кол-ва конфигурационных сообщений, которые должны посылать
узлы, чтобы узнать друг о друге. Конфигурационная информация распространяется
автоматически, как побочный эффект ключевых операций поиска. Узлы имеют достаточно знаний
и гибкости для маршрутизации запросов по путям с малыми задержками. Kademlia использует
параллельные асинхронные запросы для обхода сбойных узлов во избежание временных задержек.
Алгоритм, с помощью которого узлы узнают о существовании друг друга, устойчив к
определённым базовым отказам (?). Наконец, несколько важных свойств Kademlia формально
могут быть доказаны с помощью только некоторых предположений относительно распределения
времени безотказной работы (?). (эти допущения проверены при помощи измерений
существующих одноранговых сетей).

В Kademlia применяется базовый подход, используемый многими DHT. Kademlia использует 160 -
битные непрозрачные ключи – (?). (например, хеш SHA- 1 некоторых больших объёмов данных).
Каждый участвующий компьютер имеет идентификатор узла (node ID) в пространстве 160-битных
ключей. Пары <ключ, значение> хранятся на узлах с идентификаторами, которые более «близки»
к этим ключам по определённой метрике «близости». Наконец, алгоритм маршрутизации,
базирующийся на упомянутых идентификаторах узлов, позволяет любому участнику эффективно
находить серверы, которые «близки» к любому заданному ключу.

Многие достоинства Kademlia являются результатом применения метрики XOR для расстояния
между точками в пространстве ключей. XOR метрика симметрична, что позволяет участникам
Kademlia получать lookup запросы от точно такого же распределения узлов, которое содержится в
их таблицах маршрутизации. (Так и не понял, что хотел сказать автор.. Вообще, симметричность
метрики подразумевает одинаковое расстояние в каждую сторону между узлами, и, может быть,
из этого следует, что для общения между двумя узлами можно будет просто менять местами
отправителя и получателя и метрика будет гарантировать, что время запроса будет таким же..?).
Системам без этого свойства, например, Chord, не узнать полезной информации из запросов,
которые они получают. Более того, ассиметричность приводит к ужесточению таблиц
маршрутизации (?). Каждая запись в таблице узлов Chord должна содержать точный узел
предшествующий некоторому интервалу в пространстве идентификаторов (?). Фактически, любой
узел в интервале будет слишком далеко от узлов, предшествующих ему в этом же интервале (?).
Kademlia, напротив, может посылать запросы любым узлам в пределах интервала, что позволяет
выбирать маршруты основываясь на временной задержке или даже отправлять параллельные
асинхронные запросы нескольким одинаково подходящим по метрике узлам.

Для того, чтобы найти узлы «близкие» к конкретному ID, Kademlia использует единый алгоритм
маршрутизации от начала до конца (?). Другие системы, наоборот, используют один алгоритм для
нахождения ближайших узлов, а для последующих «прыжков» - другие(?). Из существующих
систем, Kademlia больше всего походит на первую фазу Pastry's (?), которая успешно находит узлы

примерно в 2 раза дальше от целевого ID чем XOR метрика Kademlia. Однако, во второй фазе
Pastry меняет метрику расстояния на числовую разницу между идентификаторами. Pastry также
использует вторую разностную метрику при ответе. К сожалению, узлы близкие по второй
метрике могут быть довольно далеки по первой, что создаёт разрывы при определённых значениях
идентификатора узла, снижает производительность и усложняет попытки формального анализа
наихудшего поведения.

Описание системы.
Система имеет аналогичный основной подход других DHT. Узлам назначаются непрозрачные(?)
160 - битные ID, запускается алгоритм поиска, который последовательно находит близкие к
рассматриваемому ID узлы, сходящийся к цели поиска за логарифмическое число шагов (?).

Kademlia трактует узлы как листья в бинарном дереве, где положение каждого узла определяется
по кротчайшему уникальному префиксу его ID (?). На рис. 1 показана позиция узла с уникальным
префиксом 0011 в дереве. Для любого рассматриваемого узла изначальное бинарное дерево
последовательно делится на ряд поддеревьев, которые не содержат рассматриваемый узел.
Верхнее поддерево содержит половину бинарного дерева, которая не содержит рассматриваемый
узел. Следующее поддерево состоит из половины оставшегося дерева, которая не содержит
целевой узел, и так далее. В примере с узлом 0011 обведенные деревья состоят из всех узлов с
префиксами 1, 01, 000 и 0010 соответственно.

Рис. 1. Бинарное дерево Kademlia. Чёрная точка – местоположение в дереве узла с идентификатором 0011.
Серые овалы – поддеревья, в которых должен быть по крайне мере один узел, с которым должен быть контакт
у узла 0011.

Протокол Kademlia гарантирует, что каждый узел знает, по меньшей мере, об одном из узлов в
каждом из его поддеревьев, если что(?). Благодаря этой гарантии, каждый узел может найти
любой другой узел по его ID. Рис. 2 показывает пример, когда узел 0011 ищет узел 1110 путём
последовательных запросов к узлам, которые он считает более «подходящими», для того, чтобы
найти контакты в поддеревьях спускаясь всё ниже и ниже. Наконец, результатом поиска
становится необходимый узел.

Рис. 2. Поиск узла по его ID. Узел с префиксом 0011 ищет узел с префиксом 1110, последовательно
опрашивая узлы всё ближе и ближе (?). Отрезок сверху представляет собой пространство 160-битных
идентификаторов. Он показывает, как поиск сходится к целевому узлу. Ниже показаны RPC сообщения,
сделанные узлом с идентификатором 0011. Первое RPC отправлено узлу 101, о котором узел 0011 уже знает.
Следующие RPC отправляются к узлам, возвращённым в предыдущем RPC (?).

В завершении этого раздела разъясняются детали алгоритма поиска. Сначала определяется точное
понятие близости идентификаторов, что позволяет нам говорить о хранении и поиске пар <ключ,
значение> на k ближайших узлах к рассматриваемому ключу. Далее берётся в рассмотрение
протокол поиска, который работает даже в тех случаях, когда нет узлов, которые разделили бы
свой уникальный префикс с ключом (?) или некоторые из поддеревьев, ассоциированные с
рассматриваемым узлом, являются пустыми.

2.1 XOR метрика
Каждый узел в Kademlia имеет 160-битный идентификатор. Идентификаторами узлов на данный
момент являются просто рандомные элементы 160-битного пространства ключей, хотя они могут
быть построены как в системе Chord. Кажое передаваемое узлом сообщение содержит в себе
идентификатор этого узла, что позволяет получателю запомнить «существование» отправителя,
если это необходимо.

Ключи тоже являются 160-битными идентификаторами. Для назначения пары <ключ, значение>
определённому узлу, Kademlia основывается на понятии расстояния между двумя
идентификаторами. Рассмотрим два 160-битных идентификатора, x и y, Kademlia определяет
расстояние между ними как битовое сложение по модулю два или XOR, интерпретируемое как
целое число, d(x,y) = x  y.

Прежде всего отмечается, что XOR является допустимой(?), хоть и не евклидовой метрикой.
Очевидно, что x d(x,x) = 0, x,y: x≠y d(x,y) > 0, и x,y d(x,y) = d(y,x). XOR также удовлетворяет

неравенству треугольника: d(x,y) + d(y,z) ≥ d(x,z). Неравенство треугольника следует из:
d(x,y)d(y,z)=d(x,z) и a ≥0, b≥0: a + b ≥ a  b.

Далее, отмечается, что XOR определяет понятия расстояния, которое неявно выражено в нашей
системе, основной на бинарном дереве (?). В полностью заполненном бинарном дереве
идентификаторов, величиной расстояния между двумя идентификаторами является высота
наименьшего поддерева, содержащего их обоих. Когда дерево заполнено не полностью,
ближайший к рассматриваемому идентификатору листовой узел X является узлом, чей ID
содержит самый длинный общий префикс Х. Если в дереве есть пустые ветви, может быть
несколько узлов с максимальным по длине общим префиксом. В этом случае, ближайшим к X
узлом будет узел, который является самым близким к ID ~x, который, в свою очередь, получен
переключением битов исходного X в соответствии с пустыми ветвями дерева (?).

Как и круговая метрика Chord (?), XOR является однонаправленной. Для любой точки X и
расстояния delta > 0, есть единственная точка Y такая, что d(x,y) = delta. Однонаправленность
гарантирует, что все попытки поиска одного ключа сойдутся к одному пути, независимо от узла,
инициировавшего поиск. Таким образом, кэширование пар <ключ, значение> на пути поиска
ослабляет горячие точки (?). Как и в системе Pastry (и в отличие от Chord), топология XOR
симметрична (d(x,y) = d(y,x) x,y).

2.2 Состояние узлов

Узлы Kademlia сохраняют информацию друг о друге для маршрутизации сообщений-запросов.
Для любого 0 ≤ i < 160 каждый узел хранит список троек <IP адрес, UDP порт, идентификатор
узла> для узлов, расстояние до которых между 2^i и 2^(i+1) (т.е. всего каждый узел хранит 160
списков, в каждом из которых n-ое кол-во троек?). Данные списки называются k-buckets
(вменяемого перевода я не придумал). Узлы в k-bucket хранятся в упорядоченном виде,
сортировка производится по времени (когда последний раз было общение с этим узлом) – старые
узлы – в начале, новые – в конце. Для маленьких значений i k-buckets, как правило, будут пустыми
(т.к. не будет соответствующих узлов //почему? Подразумевается, что на слишком маленьком
расстоянии не будет узлов или потому что ширина кольца маленькая для маленьких i?). Для
больших значений i размер списков может возрасти до k, где k – общесистемный параметр
репликации (?). k выбирается таким образом, что любые k узлов не будут терпеть неудачу при
передаче данных друг другу в течение часа (? очень странный параметр, кмк). Следующий пример
для k = 20?

Рис. 3. Вероятность «остаться онлайн» ещё на один час, в зависимости от времени безотказной работы. Ось
x – минуты, y – доля узлов, которые находятся онлайн минимум x минут и которые останутся онлайн ещё
минимум х + 60 минут (?).

Когда узел в Kademlia получает какое-либо сообщение (запрос или ответ) от другого узла,
обновляется соответствующий узлу-отправителю k-bucket. Если узел-отправитель уже есть в k-
bucket получателя, получатель перемещает его ID в конец листа. Если узла ещё нет в
соответствующем k-bucket и bucket имеет меньше чем k записей, получатель просто записывает
его в конец. Если соответствующий k-bucket полон, получатель пингует самый старый узел (тот,
что в начале списка) для дальнейших действий. Если этот узел не отвечает, он удаляется из k-
bucket и новый узел записывается в конец списка. Иначе, если самый «старый» узел отвечает (надо
окончательно понять, что такое least-recently и most-recently), он перемещается в конец, а узел-
отправитель отбрасывается.

k-buckets эффективно использует политику удаления последнего «увиденного» узла, за
исключением того факта, что активные узлы никогда не будут удалены из списка (так это ж
хорошо, так и задумывалось, нет?). Предпочтение старым контактам определяется анализом
данных трассировки Gnutella собранных Saroiu. Рис. 3 показывает процент узлов Gnutella, которые
остаются онлайн ещё один час, как функцию от текущего времени безотказной работы. Чем
дольше узел в сети, тем больше вероятность, что останется в сети ещё один час. Путём хранения
самых старых, но «живых» контактов, увеличивается вероятность того, что содержащиеся в k-
buckets узлы останутся в сети.

Второе достоинство k-buckets – они предоставляют защиту от некоторых DoS атак: «флуд»
новыми узлами не сбросит состояние маршрутизационных таблиц узлов. Узлы Kademlia
добавляют новые узлы в k-buckets только если старые вышли из сети.

2.3 Протокол Kademlia

Протокол Kademlia состоит из 4 RPC вызовов: PING, STORE, FIND_NODE, FIND_VALUE. PING
посылает запрос узлу и ждёт ответа, определяя тем самым, в сети ли узел. STORE указывает узлу
хранить пару <key, value> для последующего извлечения (?).

FIND_NODE принимает в качестве аргумента 160-битный ID. Получатель RPC возвращает тройки
<IP адрес, UDP порт, ID узла> для k узлов, о которых он знает и которые являются ближайшими
по отношению к целевому ID. Эти тройки могут быть получены из одного k-bucket или, если
ближайший k-bucket не заполнен полностью, из нескольких k-bucket. В любом случае, получатель
RPC должен вернуть k узлов (за исключением случая, когда во всех k-buckets взятых вместе
окажется меньше k узлов – в этом случае, получатель вернет всю информацию об узлах, которых
знает).

FIND_VALUE ведёт себя аналогично FIND_NODE – возвращает тройки <IP адрес, UDP порт, ID
узла> с одним отличием – если получатель данного RPC получил до этого STORE RPC для
данного ID узла, то он возвращает значение, соответствующее этому ключу (как я понял, в кач-ве
key <key, value> в запросе STORE используется ID узла же?).

При любом RPC вызове получатель должен повторить 160 битный случайный RPC ID, который
обеспечивает в каком-то смысле защиту от подделки адреса (?). PING’s также могут быть
дополнительно посланы на RPC-ответ RPC-получателю для получения дополнительной гарантии
сетевого адреса отправителя (?).

Самая важная процедура, которую должен выполнять участник Kademlia, - поиск k ближайших
узлов относительно определённого ID. Данная процедура называется поиск узлов (вау). В
Kademlia используется рекурсивный алгоритм поиска узлов. Инициатор поиска узлов начинает с
выбора alpha узлов из ближайшего непустого k-bucket (или, если в k-bucket содержится менее
alpha узлов, он берёт alpha ближайших узлов, о которых он знает). Далее инициатор посылает

параллельный асинхронный RPC запрос FIND_NODE alpha выбранным узлам. alpha –
общесистемный параметр «параллелизма».

На каждом шаге рекурсии, инициатор снова посылает FIND_NODE узлам, о которых он узнал на
предыдущем шаге (рекурсия может начаться до того, как все alpha первых узлов ответят на RPC).
Инициатор выбирает alpha неопрошенных узлов из k ближайших к целевому узлу, о которых он
узнал, с целью послать им FIND_NODE RPC (записи в k-bucket и ответы на FIND RPC могут быть
дополнены оценками времени прохождения запроса в обе стороны, данная информация может
быть использована при выборе alpha узлов). Узлы, которые не смогли быстро ответить на запрос,
удаляются из рассмотрения до тех пор, пока не ответят. Если очередная группа (?) RPC вызовов
FIND_NODE не смогла вернуть более близкий узел, чем уже есть, инициатор снова посылает
FIND_NODE ближайшим неопрошенным k узлам. Поиск завершается, когда инициатор опросит и
получит ответы от k ближайших узлов, которые он «увидел». При alpha = 1 алгоритм поиска
похож на Chord с точки зрения стоимости сообщения и времени обнаружения сбойных узлов.
Однако, Kademlia может выполнять маршрутизацию с меньшей задержкой, поскольку она имеет
преимущество, заключающееся в выборе любого из k узлов для пересылки запроса.

Большинство операций выполняются в пересчёте на вышеупомянутую процедуру поиска. Для
хранения пары <key, value> участник находит k ближайших к ключу узлов и отправляет им
STORE RPC. Кроме того, каждый узел повторно публикует пары <key, value> по мере
необходимости, сохраняя их «живыми» таким образом, как описано ниже в 2.5. С очень высокой
вероятностью это обеспечивает постоянство (как показано в наброске доказательства) пары <key,
value>. Для текущего приложения Kademlia (файловый обмен) также требуется чтобы
первоначальный «автор» пары <key, value> публиковал её каждые 24 часа. В противном случае,
пары <key, value> утратят силу через 24 часа после публикации в сети, с целью ограничения
устаревшего индекса в системе. Для других приложений, таких как цифровые сертификаты или
криптографические хэш-функции, может быть уместен более длительный срок действия.

Для поиска пары <key, value> узел начинает с поиска k узлов с ID, которые являются ближайшими
к целевому ключу. Однако, для поиска значений вместо FIND_NODE используется RPC-вызов
FIND_VALUE. Более того, процедура оканчивается, когда один из узлов вернёт целевое значение.
Для кэширования, как только выполнится поиск, узел, инициировавший поиск, сохраняет пару
<key, value> в ближайшем узле (по отношению к ключу), который он «видел» в процессе поиска и
который не возвращал это значение.

Из-за однонаправленности топологии, последующие поиски по тому же ключу, скорее всего,
попадут в кэшированные записи перед опросом ближайшего узла. В случае, если по
определённому ключу часто производится поиск, система могла бы его кэшировать на нескольких
узлах. Во избежание «пере-кэширования» введено время окончания срока действия пары <key,
value> в базе любого узла, которое обратно пропорционально экспоненте кол-ва узлов между
текущим узлом и узлом, чей ID ближе всего к ключу (это число может быть получено из k-bucket
текущего узла). В то время как отбрасывание по LRU (last recently used) приводит к схожему
распределению срока службы (?), нет естественного способа выбора размера кэша, поскольку узлы
не имеют априорных знаний о том, сколько значений будет хранится в системе.

k-buckets, как правило, остаются актуальными благодаря потоку запросов, проходящих через
узлы. Для обработки аномальных ситуаций, при которых нет возможности поиска для
определённого диапазона ID, каждый узел обновляет любой k-bucket, относительно которого не
был выполнена процедура поиска узла за последний час. Обновление подразумевает выбор
случайного ID в диапазоне k-bucket’а и выполнение поиска для этого ID.

Для присоединения к сети, узел u должен иметь контакт с уже участвующим узлом w. u кладёт
узел w в соответствующий k-bucket. Далее, u выполняет поиск узла для своего собственного ID.
Наконец, u обновляет все k-bucket, которые расположены дальше чем его ближайший сосед (?). Во

время обновления, u выполняет сразу две задачи: наполняет свои k-bucket и вставляет свой ID в k-
buckets других узлов, если это необходимо.

2.4 Таблица маршрутизации

Базовая структура таблицы маршрутизации Kademlia довольно проста, с учётом используемого
протокола, хотя нужна небольшая тонкость для обработки сильно несбалансированных деревьев.
Таблица маршрутизации представляет из себя бинарное дерево, листами которого являются k-
buckets. Каждый k-bucket содержит узлы с некоторым общим префиксом их ID (не совсем понял,
вроде другое определение k-bucket было). Этот общий префикс – позиция соответствующего k-
bucket в бинарном дереве. Таким образом, каждый k-bucket покрывает некоторый диапазон в
пространстве идентификаторов, и, взятые вместе k-buckets покрывают всё пространство
идентификаторов без перекрытия.

Рис. 4. Процесс изменения таблицы маршрутизации во времени. Первоначально, узел имеет один k-bucket,
как показано на верху таблицы. В процессе заполнения k-buckets, bucket, чей диапазон покрывает
идентификатор узла многократно разбивается на два k-buckets. (получается, что k-bucket содержит и себя
тоже?)

Место под узлы в дереве маршрутизации выделяется динамически, когда это требуется. На рис. 4.
показан этот процесс. Первоначально, маршрутизационная таблица узла u имеет один узел – один
k-bucket, покрывающий всё пространство идентификаторов. Получение нового контакта узлом u
приводит к попытке вставить этот контакт в соответствующий k-bucket. Если этот bucket не полон,
новый контакт просто добавляется в него. В противном случае, если диапазон k-bucket включает
собственный идентификатор узла u, тогда bucket разбивается на два новых, старое содержимое
делится между ними, затем повторяется попытка вставки. Если k-bucket с другим диапазоном (с
каким?) полон, новый контакт просто удаляется.

В сильно несбалансированных деревьях возникает одна сложность. Пусть узел u присоединяется к
системе и является единственным узлом, чей идентификатор начинается с 000. Далее,
предположим, что система уже имеет более чем k узлов с префиксом 001. Каждый узел с
префиксом 001 будет иметь пустой k-bucket, в который должен быть вставлен узел u, но
обновление bucket’а узла u приведёт только к уведомлению k узлов (что???). Во избежание этой
проблемы, узлы Kademlia хранят все валидные контакты в поддереве размером не менее k узлов,
даже если требуется разбиение k-buckets, в которых нет собственного идентификатора узла (???).
На рис. 5. показаны дополнительные разбиения. Когда узел u обновляет разделённые buckets, все
узлы с префиксом 001 узнают об этом.

Рис. 5. На рисунке изображен пример relaxed(?) таблицы маршрутизации узла, чей ID – 00..00. Relaxed
таблица может иметь небольшие (ожидается константный размер) неровностей (?) в своём разветвлении для
того, чтобы убедиться, что известны все контакты в наименьшем поддереве вокруг узла, имеющего по
крайней мере k контактов (???).

2.5 Эффективное re-publishing(?) ключей.

Для обеспечения персистентности (стойкости) пар <key, value>, узлы должны периодически
повторно публиковать (republish) ключи. С другой стороны, есть ситуации, которые могут
привести к ошибкам при поиске валидных ключей. Первая – некоторые из k узлов, которые
изначально получили пару <key, value> покинули сеть. Вторая – новые узлы могут
присоединиться к сети с ID, которые окажутся ближе к одному из обновляемых ключей чем узлы,
которые участвовали в изначальной публикации пар. В обоих случаях узлы с парами <key, value>
должны обновить их так, чтобы он присутствовал на k ближайших к ключу узлах.

Чтобы компенсировать уходящие из сети узлы, Kademlia повторно публикует раз в час каждую
пару <key, value>. Наивная реализация такой стратегии потребует множества сообщений – каждый
узел, который хранит пару <key, value> (а таких может быть максимум k), будет выполнять поиск
узла путём k- 1 RPC вызовов STORE каждый час (???). К счастью, процесс повторной публикации
хорошо оптимизируется. Во-первых, когда узел получает RPC вызов STORE для рассматриваемой
пары <key, value>, он предполагает, что RPC также было отправлено другим k- 1 ближайшим
узлам, и, следовательно, получатель (получатель RPC или ответа?) не будет повторно публиковать
пару <key, value> в течение следующего часа.

Вторая часть оптимизации позволяет избежать выполнения поиска узлов перед повторной
публикацией ключей. Как было описано в 2.4, для обработки сильно несбалансированных
деревьев, узлы по мере необходимости разбивают k-buckets, чтобы обеспечить полное знание
окружающего поддерева с не менее чем k узлами. Если перед повторной публикацией пар <key,
value> узел u обновляет все k-buckets в этом поддереве из k узлов, он автоматически сможет
определить k ближайших к данному ключу узлов. Данные обновления k-bucket могут быть
amortized сверх повторной публикации множества ключей (???).

Для того чтобы понять, почему поиск узлов излишен после того, как узел u обновил buckets в
поддереве с размером ≥ k, необходимо рассмотреть два случая. Если повторно публикуемый ключ
попадает в диапазон идентификаторов поддерева, тогда (поскольку это поддерево имеет как
минимум k элементов и у узла u есть вся необходимая информация о поддереве) очевидно, узел u
должен знать k ближайших к ключу узлов. С другой стороны, если ключ находится вне поддерева,
но узел u являлся одним из k ближайших узлов к данному ключу, (дальше вообще непонятно: это
должно следовать, что k-buckets узла u для интервалов ближе к данному ключу чем поддерево все
имеют меньше чем k записей). Следовательно, узел u будет знать о всех узлах в данных k-buckets,
которые вместе со знанием о поддереве будут включать k ближайших к ключу узлов (???????).

Когда новый узел присоединяется к системе, он должен сохранить любую пару <key, value>, ключ
которой является одним из k ближайших к узлу. Существующие узлы, при помощи аналогичного
знания о структуре окружающих их поддеревьев, будут знать какие пары должен хранить новый
узел. Любой узел, узнавший о новом узле, таким образом, пошлёт RPC вызов STORE для передачи
релевантных пар <key, value> новому узлу. Однако, чтобы избежать избыточных RPC вызовов
STORE, узел передаёт только одну пару, ключ которой ближе к его собственному
идентификатору, чем к идентификаторам других узлов (а как он это определяет? или
рассматриваются только идентификаторы его поддерева?).

Набросок доказательства (?).
Чтобы продемонстрировать правильную работу системы, необходимо доказать, что большинство
операций выполняются за [log n] + c времени, где c некоторая малая константа, и что поиск пар
<key, value> вернёт ключ, хранящийся в системе, с потрясающей  вероятностью.

Некоторые определения. Определим индекс i для k-bucket, покрывающего диапазон [2^i,2^(i+1)).
Также определим глубину дерева – h, h = 160 – i, где i это наименьший индекс непустого bucket.
Высота bucket’а узла y в узле x – это индекс bucket’а, в который узел x положит узел y минус
индекс наименее значимого (???) пустого bucket’а узла x. Поскольку идентификаторы узлов
выбираются случайно, сильно неоднородные распределения маловероятны. Таким образом,
высока вероятность того, что высота любого узла (тут про bucket? или про поддерево?) будет в
пределах константы логарифма от n (что за константа логарифма? если зафиксировать n?) для
системы с n узлами. Более того, высота bucket’а узла, который является ближайшим к
рассматриваемому ID в k-ом ближайшем узле (?), скорее всего будет в пределах log(k).

Следующий шаг – предположении об инвариантности того факта, что любой k-bucket любого узла
содержит по крайней мере один контакт, если узел существует в соответствующем диапазоне
(инвариантность относительно чего? или здесь не инвариантность? что за существование узла).
Учитывая данное предположение, можно показать, что процедура поиска узла корректна и имеет
логарифмическую сложность (время?). Пусть ближайший узел к рассматриваемому ID имеет
глубину h. Если не один из (самых значительных?) k-buckets не пуст, процедура поиска будет
уменьшать расстояние до целевого узла вдвое(?) (точнее, расстояние до которого на бит короче?)
на каждом шаге, и, следовательно, узел будет найден за h – log(k) шагов (как вообще эта формула
появилась). Если один из k-buckets узла пуст, это может быть случай, когда целевой узел
находится в диапазоне пустого блока. В этом случае, финальные шаги не будут уменьшать
расстояние до целевого узла на половину. Однако, поиск будет продолжаться точно также, как
если бы бит в ключе, соответствующий пустому k-bucket, был инвертирован. Таким образом,
алгоритм поиска всегда будет возвращать ближайший узел за h – log(k) шагов. Более того, как
только ближайший узел будет найден, параллелизм (?) изменится от alpha до k (???). Количество
шагов, необходимых для поиска оставшихся k – 1 ближайших узлов, не может быть больше, чем
высота bucket’а ближайшего узла в k-ом ближайшем узле, которая вряд ли окажется больше чем c

log(k).
Для доказательства корректности инвариантности сначала рассмотрим следствия обновления
bucket’a при условии, что инвариантность имеет место. После обновления bucket будет содержать

либо k валидных узлов, либо каждый узел в своём диапазоне, если узлов оказалось меньше, чем k
(что значит в своём диапазоне?) (Это следует из корректности процедуры поиска). Новые
присоединившиеся узлы также будут помещены в любой незаполненный bucket. Таким образом,
единственный случай, нарушающий инвариантность – это ситуация, когда существует k + 1 или
более узлов в диапазоне одного конкретного bucket’а, который уже содержит k узлов, в которых
произошел сбой (что-то про промежуточный поиск и обновления). Однако, k выбран таким
образом, чтобы уменьшить вероятность одновременного отказа в течение часа (максимальный
период обновления).

На практике, вероятность сбоя гораздо меньше, чем вероятность того, что k узлов покинут сеть в
течение часа, поскольку каждый исходящий или входящий запрос обновляет bucket узла. Это
связано с симметричностью XOR метрики, потому что идентификаторы узлов, с которыми данный
узел взаимодействует во время исходящего или входящего запроса, совместимы с диапазонами
bucket’а узла (?).

Более того, даже если инвариантность не выполняется для одного bucket’а в одном узле, это
повлияет только на время работы (добавив прыжок(?) некоторым процедурам поиска), на
корректности поиска узлов это не отразится. Для того, чтобы поиск закончился неудачей, каждый
из k узлов на пути поиска должен потерять k узлов в одинаковых bucket и не иметь
промежуточных процедур поиска и обновлений (т.е. в bucket этих узлов должны быть мёртвые
узлы и рассматриваемые узлы не должны обновлять свои bucket, чтобы не удалить эти мёртвые
узлы?). Если bucket’ы разных узлов не перекрываются, это может произойти с вероятностью 2^(-
k^2). С другой стороны, узлы, появляющиеся в множестве других buckets, скорее всего будут
иметь более длительное время работы, и, следовательно, уменьшат вероятность отказа.

Рассмотрим восстановление пары <key, value>. Когда пара <key, value> опубликована, она
помещается в k ближайших к ключу узлов. Также она повторно обновляется каждый час.
Поскольку даже новые узлы (наименее надёжные) имеют вероятность ½ от прошедшего часа (???),
по прошествии часа, пара <key, value> пара будет присутствовать в одном из k узлов с
вероятностью 1 – 2^(-k). При добавлении новых узлов, которые близки к ключу, это свойство не
нарушается, потому что как только такой узел будет добавлен, узлы связываются со своими
ближайшими узлами для того, чтобы заполнить их buckets и, тем самым, получить какие-либо
пары <key, value> для хранения (?). Конечно, если k ближайших к ключу узлов сбойные и пара
<key, value> больше нигде не хранится, Kademlia не сможет сохранить эту пару, и, следовательно,
потеряет ключ.

Замечания реализации.
В этом разделе описываются две важные техники, используемые для улучшения
производительности Kademlia.

4.1. Оптимизированный учёт контактов (?).

Основное желаемое свойство k-buckets – обеспечить LRU проверку и отброс невалидных
контактов без отбрасывания валидных (полнота и точность?). Как было описано в 2.2, если k-
bucket полон, требуется посылать RPC вызов PING каждый раз, когда приходит новое сообщение
от неизвестного узла в диапазоне этого bucket’a. PING используется для того, чтобы проверить,
является ли валидным LRU контакт в k-bucket. Если это не так, новый контакт занимает место
старого. К сожалению, описанный алгоритм требует большого количества сетевых сообщений
PING.

Для уменьшения трафика, Kademlia задерживает probing контактов до тех пор, пока у них не
появятся полезные сообщения для отправки (?). Когда узел Kademlia получает RPC от
неизвестного узла и k-bucket для данного узла уже имеет k записей, узел помещает новый контакт
в кэш замещения для узлов, которые могут заменить устаревшие в k-bucket. В следующий раз,
когда узел опрашивает контакты в k-bucket, любой неответивший узел может быть удалён и на его

место помещён узел из кэша замещения. Кэш замещения отсортирован по времени, наиболее
недавно использовавшаяся запись имеет более высокий приоритет как кандидат на помещение в k-
bucket.

Связанная проблема состоит в том, что Kademlia использует UDP, валидные контакты иногда
будут не в состоянии ответить, т.к. сетевой пакет будет потерян. Т.к. потеря пакетов часто
указывает на перегрузку сети, Kademlia блокирует неотвечающие узлы и позволяет избежать
отсылки данным узлам дальнейших RPC для экспоненциального роста backoff интервала (?).
Поскольку на большинстве этапов поиска необходимо поддерживать связь только с одним из k
узлов, система обычно не передаёт повторно потерянные RPC тому же узлу.

Когда узел не смог ответить на 5 RPC вызовов подряд, он считается устаревшим. Если k-bucket не
полон или кэш замещения пуст, Kademlia просто помечает узел как устаревший, а не удаляет его.
Среди прочего, это обеспечивает то, что если сетевое соединение узла временно отсутствует, он не
будет удалён из всех k-buckets.

4.2. Ускорение поиска
Ещё один аспект оптимизации заключается в уменьшении переходов при поиске путём
увеличения размера таблицы маршрутизации. В целом, это достигается путём рассмотрения сразу
же b бит идентификатора одновременно. Как было описано ранее, ожидаемое количество
переходов за одну процедуру поиска – log 2 (n). С помощью увеличения размера таблицы
маршрутизации до 2blog 2 bn k-buckets (?), можно уменьшить количество ожидаемых переходов до
log 2 bn.

В разделе 2.4 описывается, как Kademlia узлы делят k-bucket когда он полон и его диапазон
включает собственный идентификатор узла. Однако, реализация также делит диапазоны, не
содержащие идентификатор узла, вплоть до b- 1 уровней. Например, если b = 2, половина
пространства идентификаторов, не содержащая идентификатор узла, распадается на два
диапазона; при b = 3, оно распадается на 2 уровня диапазонов, всего максимум на 4 диапазона.
Главное правило разбиения состоит в том, что узел разбивает полный k-bucket, если диапазон k-
bucket’a содержит собственный идентификатор узла или глубина k-bucket’a d в дереве
маршрутизации удовлетворяет условию d ≠ 0 (mod b) (d не делится на b?). (Глубина — это длина
префикса, являющегося общим для всех узлов в диапазоне k-bucket’a). В текущей реализации
используется b = 5.

XOR маршрутизация напоминает первый этап алгоритмов маршрутизации Pastry, Tapestry и
Plaxton, однако все эти алгоритмы усложняются при переходе к b > 1. Без использования метрики
XOR возникает необходимость в использовании дополнительной структуры для обнаружения
цели в узлах, имеющих один и тот же префикс, но отличающихся в следующем b-битном числе
(имеется отличающиеся по следующим b битам?). Все три алгоритма решают данную проблему
по-разному, каждый со своим недостатком; все они требуют второстепенные таблицы
маршрутизации размером O(2b) в дополнение к главным таблицам размером O( 2 blog 2 bn). Это
увеличивает стоимость загрузки и обслуживания, усложняет протоколы, а в случае с Pastry и
Tapestry – усложняет или вовсе препятствует формальному анализу корректности и
согласованности. У Plaxton есть доказательство, но такая система менее ориентирована на
отказоустойчивые среды, такие как одноранговые сети.

Заключение
Благодаря новой топологии, основанной на XOR метрике, Kademlia является первой системой
одноранговой сети, имеющая доказуемую согласованность и производительность, маршрутизацию
с минимальными задержками и симметричную однонаправленную топологию (?). Более того,
Kademlia вводит параметр параллелизма alpha, который позволяет людям trade постоянным
коэффициентом в пропускной способности для асинхронного выбора перехода с наименьшей
задержкой и восстановления после сбоя без задержки. Kademlia первая одноранговая система,

использующая тот факт, что вероятность сбоя узла обратно пропорциональна времени безотказной
работы.
